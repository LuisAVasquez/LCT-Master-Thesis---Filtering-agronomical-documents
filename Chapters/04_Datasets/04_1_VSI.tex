%\customHeader{1}{\VSI{} Dataset}
%\label{vsi_dataset}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\customHeader{1}{Dataset Collection}
\label{vsi_data_collection}


\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{Figures/04/04_vsi_dataset_collection.png}
    \caption{\VSI{} Data Collection Pipeline}
    \label{fig:04_vsi_data_collection_pipeline}
\end{figure}


The main tool for collecting documents is the web scrapping tool \emph{ScaleSERP}\footnote{\url{https://www.scaleserp.com/}}. Every week, several queries are sent to various services, including the Google Search Engine, using the keywords and key phrases related to pathogens of interest (Table \ref{tab:04_query_keywords}).%, using a Python script. 
%The results are then fed to the text-mining tool \emph{PADI-Web} \myparencite{padiweb3}, in order to retrieve information about the document's source, publication date, entities mentioned, locations and more. After extraction, this data is stored in a database and subsequently analysed by the \gls{vsi} experts to assess their relevance for monitoring. 


As part of the \gls{vsi} pipeline, shown in Figure \ref{fig:04_vsi_data_collection_pipeline}, the websites corresponding to the URLs obtained from \emph{ScaleSERP} are downloaded using a Python script. 
For each website, its source (in HTML) is downloaded, and the tool \trafilatura{} \myparencite{barbaresi-2021-trafilatura} parses it to XML/TEI, while attempting to discard ``non-contents", such as menus, headings, ads, etc.
Then, \trafilatura{} takes the XML/TEI and attempts to extract a title and  an abstract from the metadata\footnote{The title and the abstract are extracted from the \texttt{metadata.title} and \texttt{metadata.description} fields, respectively.} and the full text from the XML body.

%Then, they are scrapped using the tool \trafilatura{}.
%the documents  are scrapped using the tool \trafilatura{} \myparencite{barbaresi-2021-trafilatura}, which attempts to extract a title, an abstract, the full text, and other data from the websites.
%Specifically, for a website, \trafilatura{} scrapes the HTML content and parses it into XML. The tool then extracts the title and abstract from the XML metadata\footnote{The title and the abstract are extracted from the \texttt{metadata.title} and \texttt{metadata.description} fields, respectively.} and the full text from the XML body.

Given that these articles are extracted from public websites, they remain in their original language. The Google Translate API is used to assist the annotators with foreign-language articles. This API also automatically detects the document's language.
Due to budget constraints, only the \trafilaturaTitle{} is translated into English.


%Given that these articles come from public websites, all the content is in its original language.
%In order to help the annotators make a decision when encountering an article in a foreign language, and due to budget limitations, only the title is translated to English, using the Google Translate API, which also provides an automatically detected language for the document. 

Since documents are added each week, the \gls{vsi} database is in constant construction. 
For our work, we utilized with the documents collected from the 11th of July 2022 to the 16th of April 2023, that is, the data collected over 39 weeks, comprising approximately 35,000 documents, which corresponds to more than 800 documents per week. We focus on a subset of the \VSI{} database, specifically its textual data, to obtain the \VSI{} dataset  (Table \ref{tab:04_pesv_sources of content}).

\input{Tables/04/Sources_of_content_trafilatura_google_translate}



% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\customHeader{1}{Dataset Annotation}
\label{vsi_data_annotation}

\putInBox{
There are four \gls{vsi} experts dedicated to annotating the dataset.
Regrettably, while constructing the dataset, there was no record maintained to track the identity of the annotator responsible for labelling each document. For this reason, we are not able to provide any inter-annotator agreement measures on this dataset \myparencite{agreement_measures}.
}



%\begin{itemize}
%    \item \todo{add names of \gls{vsi} experts}
%    \item \todo{add names of \gls{vsi} experts}
%    \item \todo{add names of \gls{vsi} experts}
%    \item \todo{add names of \gls{vsi} experts}
%\end{itemize}


%After obtaining results from the Google Search, the \gls{vsi} experts are presented with the URL of a document, which they manually open and proceed to read. Using the interface in Figure \ref{fig:04_pesv_interface_1}, they fill out the fields \emph{Titre} (title), \emph{Auteurs} (authors), \emph{Organism nuisible} (pathogen), \emph{Sujet} (subject), \emph{Fiabilité} (reliability), etc. 

For annotation, the results of the \gls{vsi} pipeline mentioned above are shared with the \gls{vsi} experts. They utilize the interface shown in Figure \ref{fig:04_pesv_interface_1} to inspect the results for various fields, including the \emph{Date de publication} (publication date), \emph{Auteurs} (authors), \emph{Organism nuisible} (pathogen), etc. Additionally, they manually input information in other fields such as \emph{Sujet} (subject), \emph{Fiabilité} (reliability), and come up with a \emph{Titre} (title).
It is important to note that the title written by the \gls{vsi} experts is usually different from the one extracted by \trafilatura{} (for example, 
%they may include different characters
the wording may vary). 
%Other fields, like \emph{Date de publication} (publication date) and \emph{Lien} (link) are filled automatically.

After inspection, we found that that annotators exclusively provide titles for articles they find relevant, that is, there are no titles for irrelevant articles. Since our objective is to automate the process of identifying relevant articles, we disregard the titles provided by the annotators. Instead, we rely solely on the titles generated by \trafilatura{} and Google Translate, which are stored even in the case of the documents being rejected.

Special attention must be given to the \emph{Sujet} (subject), as this field will be crucial for preprocessing the dataset (see \headerName{} \ref{vsi_resolving_inconsistencies}).

\putInBox{
%As part of their monitoring mission, the \gls{pesv} Platform surveys any health risk or plant health phenomenon that has or may have an impact on agriculture. 
When the \gls{vsi} experts encounter an event that catches their attention (that is, that they consider \textbf{relevant}), they assign a \textbf{subject} to the respective document and utilize the interface presented in Figure \ref{fig:04_pesv_interface_2} to allocate a subject ID and description to the document.
Each subject corresponds to a specific health risk event that was reported during the weeks before the experts reviewed the documents.
It is possible for multiple documents to share the same subject. In the case of documents considered \textbf{irrelevant} to the monitoring of plant health, the subject field for the document is left empty.
}

Subsequently, after a document is deemed relevant by the \gls{vsi} experts, it is officially published in the bulletin of the  \gls{pesv} Platform\footnote{\url{https://plateforme-esv.fr/bulletins_et_points_sur_VSI}} and can be accessed through the \gls{vsi} Document Search Engine\footnote{\url{https://plateforme-esv.fr/moteur-de-recherche-vsi}}. Some sample entries containing all sources of content can be found in Table \ref{tab:04_sample_entries_vsi}.


\input{Tables/04/Sample_entries}


\begin{landscape}
    \begin{figure}[ht]
        \centering
        \includegraphics[width=1.20\textwidth]{Figures/04/PESV_interface_1.png}
        \caption{\VSI{} Annotation Interface for Articles}
        \label{fig:04_pesv_interface_1}
    \end{figure}
\end{landscape}

\begin{landscape}
    \begin{figure}[ht]
        \centering
        \includegraphics[width=1.20\textwidth]{Figures/04/PESV_interface_2.png}
        \caption{\VSI{} Annotation Interface for Subjects}
        \label{fig:04_pesv_interface_2}
    \end{figure}
\end{landscape}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\customHeader{1}{Dataset Stastitics}
\label{vsi_data_statistics}

There are 34,587 entries in the dataset, out of which 3,715 have an assigned subject and 30,872 do not, producing a highly unbalanced dataset (Figure \ref{fig:04_naive_positives_and_negatives}). 

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/04/naive_positives_and_negatives_chart.png}
    \caption{Distribution of subjects in the \VSI{} dataset without preprocessing}
    \label{fig:04_naive_positives_and_negatives}
\end{figure}

The language automatically detected by the Google Translate API on the \trafilaturaTitle{} can be used to approximately study the distribution of the entries in the dataset. In Figure \ref{fig:04_vsi_language_distribution} it can be seen that around one third of the entries are in English, followed by Italian, Spanish, and French. There is a considerable amount of failed translation attempts, for around one fifth of the entries. The complete data on documents per language is available in Appendix \ref{appendix01:vsi_language_distribution}.
 
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Figures/04/Entries by detected language.png}
    \caption{Language distribution of entries in the \VSI{} dataset}
    \label{fig:04_vsi_language_distribution}
\end{figure}

The presence of some Latin documents is surprising. Upon inspecting the data, we discovered that this occurrence was due to scrapped \trafilaturaTitle{}s with several scientific names (Table \ref{tab:appendix01:latin_entries} in the Appendix).

Often, the \trafilatura{} scrapper is not able to retrieve content for the \trafilaturaTitle{}, \trafilaturaAbstract{}, and \trafilaturaFulltext{} of a website, or the Google Translate API fails. Thus, there are several entries without content for all fields. If we check the unique entries for each source of content, we can see that there are numerous duplicate entries, since there are at least around 20,000 unique entries per source of content (Figure \ref{fig:04_unique_entries_vsi}).

\begin{figure}
    \centering
    \includegraphics[width=0.50\textwidth]{Figures/04/Unique entries per content source.png}
    \caption{Unique entries per source of content in the \VSI{} dataset}
    \label{fig:04_unique_entries_vsi}
\end{figure}

As the\translationTitle{} is derived from the \translationTitle{}, and due to the possibility of failure in the Google Translate API, the number of unique entries for the \translationTitle{} is lower than that for the \trafilaturaTitle{}.
%As one would expect, there are fewer unique entries for the \translationTitle{} as there are for the \trafilaturaTitle{}. 
At the same time, the \trafilaturaAbstract{} field has fewer unique entries than all the other \contentType{}s, while the \trafilaturaFulltext{} has the most. This may be explained by the fact that, usually, the creators of a website omit filling the metadata, and when they do, they frequently include only a title and not a description, while most times, the body of the website will be available. 

Using naive tokenization (splitting on whitespace), we can study the distribution of the tokens for all sources of content. In the histograms in Figure \ref{fig:04_vsi_token_distribution} the longest documents have been grouped together to facilitate plotting \footnote{This explains the long bars at the end of the right tails of the histograms.}. As one can see, the \trafilaturaTitle{} and \translationTitle{} have a very similar distribution, both are shorter than the \trafilaturaAbstract{}, while the content from the \trafilaturaFulltext{} is the longest.

\begin{figure}[ht]
    \centering
    \subfigure[\trafilaturaTitle{}]{
        \includegraphics[width=0.45\textwidth]{Figures/04/Histograms/length histogram parsed_trafilatura_title.png}
        %\label{fig:image1}
    }
    \hfill
    \subfigure[\trafilaturaAbstract{}]{
        \includegraphics[width=0.45\textwidth]{Figures/04/Histograms/length histogram parsed_trafilatura_abstract.png}
        %\label{fig:image2}
    }
    
    \subfigure[\trafilaturaFulltext{}]{
        \includegraphics[width=0.45\textwidth]{Figures/04/Histograms/length histogram parsed_trafilatura_fulltext.png}
        %\label{fig:image3}
    }
    \hfill
    \subfigure[\translationTitle{}]{
        \includegraphics[width=0.45\textwidth]{Figures/04/Histograms/length histogram translation_title.png}
        %\label{fig:image4}
    }
    
    \caption{
    Length (in n. of tokens) of the \contentType{}s in the \VSI{} dataset
    }
    \label{fig:04_vsi_token_distribution}
\end{figure}







% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\customHeader{1}{Dataset Issues}
\label{vsi_data_issues}


Upon examining the data, we encountered numerous issues that we proceed to describe and which will be addressed during preprocessing (\headerName{} \ref{vsi_preprocessing}).


\customHeader{2}{Duplicate Entries}
\label{vsi_issues_duplicates}
\ \\

As mentioned in \headerName{} \ref{vsi_data_statistics}, there are some duplicate entries in the dataset (Figure \ref{fig:04_naive_positives_and_negatives} VS Figure \ref{fig:04_unique_entries_vsi}). According to the \gls{vsi} experts, often the Google Search for a query will return the same results from one week to the next, and this introduces duplicate entries.
 


\customHeader{2}{Scrapping Failures}
\label{vsi_issues_error_messages}
\ \\

On some instances, the \trafilatura{} scrapper either fails or is blocked by website servers that do not allow bots. This introduces error messages to the dataset, some samples of which can be seen in Table \ref{tab:04_error_messages}.

\input{Tables/04/Error_messages}



\customHeader{2}{Scrapping errors}
\label{vsi_issues_scrapping_errors}
\ \\

In some other instances, the web scraper successfully extracts content from the website, but the parsing algorithm performs poorly, retrieving only the headers or the website's name (Tables \ref{tab:04_headers} and \ref{tab:04_website_names}).

\input{Tables/04/Headers_and_websites}

This type of scrapping error introduces inconsistencies into the \VSI{} dataset, as multiple documents end up having the same text for several \contentType{}s, as in Table \ref{tab:04_xmol_inconsistencies}\footnote{\href{https://www.x-mol.com/}{X-MOL} is a Chinese search engine for scholars.}.

%This introduces inconsistencies into the \VSI{} dataset. In cases where multiple documents originate from the same problematic website, the original content is lost, and we encounter entries that contain identical text but carry different assigned subjects, as in Table \ref{tab:04_xmol_inconsistencies}\footnote{\href{https://www.x-mol.com/}{X-MOL} is a Chinese search engine for scholars.}.




\input{Tables/04/X-MOL_problem}




\customHeader{2}{Data Noise}
\label{vsi_issues_data_noise}
\ \\

Even in those cases when \trafilatura{} is able to retrieve content from the website, there are some expected types of noise in the data, considering it comes from the internet.
Namely, the following  (Table \ref{tab:04_noise}) :
\begin{itemize}
    \item URLs
    \item HTML Tags
    \item Emojis
    \item Encoding errors
    \item Generic noise (different quotation or division characters, strings containing only dates, etc\ldots)
\end{itemize}




% endings - pubmed
% URLs, tags, emojis


\input{Tables/04/Noise}


Furthermore, there exists a source of noise that may not be immediately apparent: occasionally, the content concludes with the website's name, leading to different entries that vary by only a few tokens as a suffix (Table \ref{tab:04_website_names_as_suffixes}).

\input{Tables/04/Similar_entries}


\customHeader{2}{Annotation Inconsistencies}
\label{vsi_issues_annotation_inconsistencies}
\ \\

Finally, human error also introduces inconsistencies into the \VSI{} dataset. In some cases, there are entries with the exact same content, with different or no assigned subjects (Table \ref{tab:04_annotation_inconsistencies}). After consulting with the \gls{vsi} experts, they explained that this is due to two factors:

\begin{itemize}
    \item Different annotators labeling the same content.
    \item After a subject loses relevance (see \headerName{} \ref{vsi_data_collection}), the annotators usually ignore articles related to it, which is effectively equivalent to assigning no subject.
\end{itemize}


\input{Tables/04/Annotation_inconsistencies}


\putInBox{
Due to all the issues mentioned, one must take the statistics in \headerName{} \ref{vsi_data_statistics} with a grain of salt. In particular, naively checking the entries for the presence of an assigned subject (as in Figure \ref{fig:04_naive_positives_and_negatives}) may lead to very similar or exactly equal content labelled in two different categories, thus rendering the classification task significantly more challenging. These issues will addressed during preprocessing (\headerName{} \ref{vsi_preprocessing}).
}



% making sure to print all tables and figures 
\clearpage