\customHeader{1}{Evaluation Metrics for Classification}
\label{02_evaluation_metrics}

After training a classifier for a Classification task, including \textclassification{}, the subsequent phase is to assess its performance. 
This involves leveraging a dataset with predefined class labels and contrasting the classifier's \emph{predicted} with these \emph{true} labels.

\appendixname{} \ref{appendix00:metrics} contains the motivations and definitions for five classic metrics for evaluating binary classifiers: accuracy, precision, recall, \fOne{}, and \auc{}.
In this section, we take a step further and discuss the \fBeta{} score. The \fBeta{} score extends the concept of \fOne{} by introducing a parameter $\beta$, which allows us to control the trade-off between precision and recall. A higher value of beta emphasizes recall, while a lower value emphasizes precision. The \fOne{} score is a special case of the \fBeta{} score when $\beta$ is set to 1.


\begin{align}
    F_\beta &= (1+\beta^2) \dfrac{P * R}{\beta^2* P + R} \label{eq:def_f_beta}
\end{align}


Example: If $\beta$ is set to $2$, the \fTwo{} score will give more two times more weight to recall than precision, making it suitable for tasks where recall is more important than precision, such as ours.

\putInBox{
Our task involves distinguishing between events representing health risks and impacting agriculture (the positive class, ``Relevant") from harmless events (the negative class, ``Irrelevant"). In this context, it becomes vital to identify as many true positive cases as possible, as they represent actual risks with potential consequences for agriculture. 
%We place a higher priority on this aspect, even if it leads to some harmless events being missclassified.
%This is because, o

\vspace{10pt}

Once our system flags a potential risk, subject experts will be notified to make the final assessment. 
A low precision would indicate that only a small fraction of the documents presented to the experts are actually relevant. Conversely, a low recall would imply that many actually relevant documents would be overlooked by the system, requiring experts to reexamine and asses the original data, a costly and time consuming task.

\vspace{10pt}

 As a result, our focus leans more towards improving recall over precision. Therefore, we choose to assess our system using the \fTwo{} score.
}


