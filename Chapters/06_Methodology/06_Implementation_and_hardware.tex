\customHeader{1}{Implementation, Hyperparameters, and Hardware}
\label{06_implementation_and_hardware}

For this work, we worked in Python 3.9.2 and implemented a package for preprocessing the datasets and training \gls{bert} language models. This package is built around the \texttt{Transformers} library from the \emoji{hugging-face} HuggingFace company  for \finetuning{} \myparencite{transformers_hf_library}, \texttt{Scikit-learn} for metrics and statistics \myparencite{scikit-learn}, \texttt{pandas} for handling the datasets \myparencite{pandas}, \texttt{BeautifulSoup4} for parsing XML \myparencite{beautiful_soup_4}, among others. We also modified the code repository for \gls{pet} to include our own task-specific patterns and verbalizers.  Our implementation is designed to run on one GPU.


% Beautiful soup
% hugging face
% sci kit learn
% preprocessing and classification REPO
% PET repo

\input{Tables/06/Repositories}


During training, we keep track of the accuracy, precision, recall, \fOne{}, and \fTwo{} on the training and development splits per epoch. For the final models, we calculate all these metrics along with the \gls{auc} on the development and test splits. Memory constraints prevented us from saving model checkpoints during training. Therefore, we conducted two training rounds: one to determine the best number of epochs and another to train only up to those identified optimal epochs.


% Maiage computers
% LabIA cluster

We performed some experiments on the \MAIAGE{} local computers, but the bulk of the training was performed on the \href{http://hebergement.universite-paris-saclay.fr/lab-ia/}{\texttt{Lab-IA}} cluster\footnote{\url{http://hebergement.universite-paris-saclay.fr/lab-ia/}}. This is a computing platform associated to the University of Paris-Saclay, and serves researchers in the Paris region. For our experiments, we conducted various tests and executed our code, but the GPU selection was beyond our control, contingent on the availability of GPUs. Table \ref{tab:06_labia_specs} shows the specifications of this cluster.




After several preliminary experiments, it became clear that the size of our datasets and the implementations of our training methods (specifically, \gls{pet}) would pose a challenge for the available hardware. 
These preliminary tests also shed light on certain patterns in the training results, which informed our hyperparameter choices.
We found that \finetuning{} was faster than \gls{pet}, that larger batch sizes maxed out our GPU memory, and that extremely high or low learning rates led to erratic learning.
Table \ref{tab:06_hyperparameters} lists the hyperparameters we used for our final setup.
For any hyperparameters not highlighted in this table, we relied on the default settings offered by the \texttt{Transformers} and the original \gls{pet} implementations.


% input size % PET patterns.
% batch size
% learning rate
% epochs
% optimizer

\input{Tables/06/hyperparameters}





