\begin{table}%[]
    \centering
    \begin{tabular}{p{4cm}|p{8cm}}
        \textbf{Activation Function} &  \textbf{Description} \\ \hline
        Identity &  No modification to the weighted sum\\
        \gls{relu} & Eliminates negative values. Better for optimization \\
        \gls{sigmoid} & Outputs a single probability \\
        \gls{softmax} & Generalization of \gls{sigmoid} for multiple categories. Outputs a list of probabilities. \\
        \gls{tanh} & Reduces the influence of huge and minuscule values. Better for optimization \\
    \end{tabular}
    \caption{Common activation functions}
    \label{tab:02_nn_common_activation_functions}
\end{table}
