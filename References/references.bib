@inproceedings{2020_what_happens_to_bert_embeddings_during_finetuning,
    title = "What Happens To {BERT} Embeddings During Fine-tuning?",
    author = "Merchant, Amil  and
      Rahimtoroghi, Elahe  and
      Pavlick, Ellie  and
      Tenney, Ian",
    booktitle = "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.blackboxnlp-1.4",
    doi = "10.18653/v1/2020.blackboxnlp-1.4",
    pages = "33--44",
    abstract = "While much recent work has examined how linguistic information is encoded in pre-trained sentence representations, comparatively little is understood about how these models change when adapted to solve downstream tasks. Using a suite of analysis techniques{---}supervised probing, unsupervised similarity analysis, and layer-based ablations{---}we investigate how fine-tuning affects the representations of the BERT model. We find that while fine-tuning necessarily makes some significant changes, there is no catastrophic forgetting of linguistic phenomena. We instead find that fine-tuning is a conservative process that primarily affects the top layers of BERT, albeit with noteworthy variation across tasks. In particular, dependency parsing reconfigures most of the model, whereas SQuAD and MNLI involve much shallower processing. Finally, we also find that fine-tuning has a weaker effect on representations of out-of-domain sentences, suggesting room for improvement in model generalization.",
}


@article{kim-etal-2019-categorical_metadata_text_classification,
    title = "Categorical Metadata Representation for Customized Text Classification",
    author = "Kim, Jihyeok  and
      Amplayo, Reinald Kim  and
      Lee, Kyungjae  and
      Sung, Sua  and
      Seo, Minji  and
      Hwang, Seung-won",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1013",
    doi = "10.1162/tacl_a_00263",
    pages = "201--215",
    abstract = "The performance of text classification has improved tremendously using intelligently engineered neural-based models, especially those injecting categorical metadata as additional information, e.g., using user/product information for sentiment classification. This information has been used to modify parts of the model (e.g., word embeddings, attention mechanisms) such that results can be customized according to the metadata. We observe that current representation methods for categorical metadata, which are devised for human consumption, are not as effective as claimed in popular classification methods, outperformed even by simple concatenation of categorical features in the final layer of the sentence encoder. We conjecture that categorical features are harder to represent for machine use, as available context only indirectly describes the category, and even such context is often scarce (for tail category). To this end, we propose using basis vectors to effectively incorporate categorical metadata on various parts of a neural-based model. This additionally decreases the number of parameters dramatically, especially when the number of categorical features is large. Extensive experiments on various data sets with different properties are performed and show that through our method, we can represent categorical metadata more effectively to customize parts of the model, including unexplored ones, and increase the performance of the model greatly.",
}




@inproceedings{match_metadata_for_multilabel_classification,
author = {Zhang, Yu and Shen, Zhihong and Dong, Yuxiao and Wang, Kuansan and Han, Jiawei},
title = {MATCH: Metadata-Aware Text Classification in A Large Hierarchy},
year = {2021},
isbn = {9781450383127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442381.3449979},
doi = {10.1145/3442381.3449979},
abstract = {Multi-label text classification refers to the problem of assigning each given document its most relevant labels from a label set. Commonly, the metadata of the given documents and the hierarchy of the labels are available in real-world applications. However, most existing studies focus on only modeling the text information, with a few attempts to utilize either metadata or hierarchy signals, but not both of them. In this paper, we bridge the gap by formalizing the problem of metadata-aware text classification in a large label hierarchy (e.g., with tens of thousands of labels). To address this problem, we present the MATCH1 solution—an end-to-end framework that leverages both metadata and hierarchy information. To incorporate metadata, we pre-train the embeddings of text and metadata in the same space and also leverage the fully-connected attentions to capture the interrelations between them. To leverage the label hierarchy, we propose different ways to regularize the parameters and output probability of each child label by its parents. Extensive experiments on two massive text datasets with large-scale label hierarchies demonstrate the effectiveness of MATCH over the state-of-the-art deep learning baselines.},
booktitle = {Proceedings of the Web Conference 2021},
pages = {3246–3257},
numpages = {12},
keywords = {academic graph, hierarchical classification, text classification},
location = {Ljubljana, Slovenia},
series = {WWW '21}
}

@inproceedings{model_cards_for_model_reporting,
  title="Model cards for model reporting",
  author={Mitchell, Margaret and Wu, Simone and Zaldivar, Andrew and Barnes, Parker and Vasserman, Lucy and Hutchinson, Ben and Spitzer, Elena and Raji, Inioluwa Deborah and Gebru, Timnit},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={220--229},
  year={2019},
  url = {https://doi.org/10.1145/3287560.3287596},
  doi = {10.1145/3287560.3287596},
}

@article{text_classification_embeddings_survey,
  title={Text classification using embeddings: a survey},
  author={da Costa, Liliane Soares and Oliveira, Italo L and Fileto, Renato},
  journal={Knowledge and Information Systems},
  pages={1--43},
  year={2023},
  publisher={Springer}
}

@inproceedings{evaluation_metrics_document_filtering,
  title="A Comparison of Evaluation Metrics for Document Filtering.",
  author={Amig{\'o}, Enrique and Gonzalo, Julio and Verdejo, Felisa},
  booktitle={CLEF},
  pages={38--49},
  year={2011},
  organization={Springer}
}

@inproceedings{probabilistic_interpretation_precision_recall_fscore,
  title="A probabilistic interpretation of precision, recall and F-score, with implication for evaluation",
  author={Goutte, Cyril and Gaussier, Eric},
  booktitle={Advances in Information Retrieval: 27th European Conference on IR Research, ECIR 2005, Santiago de Compostela, Spain, March 21-23, 2005. Proceedings 27},
  pages={345--359},
  year={2005},
  organization={Springer}
}



@BOOK{fbetascore,
  title     = "Information Retrieval",
  author    = "Van Rijsbergen, C J",
  publisher = "Butterworth-Heinemann",
  edition   =  2,
  month     =  jan,
  year      =  1979,
  address   = "Oxford, England"
}

@inproceedings{BioMedicalBERT_ALBERT_ELECTRA_2021,
    title = "{B}io{M}-Transformers: Building Large Biomedical Language Models with {BERT}, {ALBERT} and {ELECTRA}",
    author = "Alrowili, Sultan  and
      Shanker, Vijay",
    booktitle = "Proceedings of the 20th Workshop on Biomedical Language Processing",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.bionlp-1.24",
    doi = "10.18653/v1/2021.bionlp-1.24",
    pages = "221--227",
    abstract = "The impact of design choices on the performance of biomedical language models recently has been a subject for investigation. In this paper, we empirically study biomedical domain adaptation with large transformer models using different design choices. We evaluate the performance of our pretrained models against other existing biomedical language models in the literature. Our results show that we achieve state-of-the-art results on several biomedical domain tasks despite using similar or less computational cost compared to other models in the literature. Our findings highlight the significant effect of design choices on improving the performance of biomedical language models.",
}


@inproceedings{BioNER_with_multilingualBERT_2019,
    title = "Biomedical Named Entity Recognition with Multilingual {BERT}",
    author = "Hakala, Kai  and
      Pyysalo, Sampo",
    booktitle = "Proceedings of the 5th Workshop on BioNLP Open Shared Tasks",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5709",
    doi = "10.18653/v1/D19-5709",
    pages = "56--61",
    abstract = "We present the approach of the Turku NLP group to the PharmaCoNER task on Spanish biomedical named entity recognition. We apply a CRF-based baseline approach and multilingual BERT to the task, achieving an F-score of 88{\%} on the development data and 87{\%} on the test set with BERT. Our approach reflects a straightforward application of a state-of-the-art multilingual model that is not specifically tailored to either the language nor the application domain. The source code is available at: https://github.com/chaanim/pharmaconer",
}




@misc{BERT_paper,
      title="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{linkbert_biolinkbert,
      title="LinkBERT: Pretraining Language Models with Document Links", 
      author={Michihiro Yasunaga and Jure Leskovec and Percy Liang},
      year={2022},
      eprint={2203.15827},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{scibert,
    title = "{S}ci{BERT}: A Pretrained Language Model for Scientific Text",
    author = "Beltagy, Iz  and
      Lo, Kyle  and
      Cohan, Arman",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1371",
    doi = "10.18653/v1/D19-1371",
    pages = "3615--3620",
    abstract = "Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.",
}


@misc{roberta,
  abstract = {Language model pretraining has led to significant performance gains but
careful comparison between different approaches is challenging. Training is
computationally expensive, often done on private datasets of different sizes,
and, as we will show, hyperparameter choices have significant impact on the
final results. We present a replication study of BERT pretraining (Devlin et
al., 2019) that carefully measures the impact of many key hyperparameters and
training data size. We find that BERT was significantly undertrained, and can
match or exceed the performance of every model published after it. Our best
model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results
highlight the importance of previously overlooked design choices, and raise
questions about the source of recently reported improvements. We release our
models and code.},
  added-at = {2020-12-11T12:52:54.000+0100},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  biburl = {https://www.bibsonomy.org/bibtex/2a4c60811a43da7596716d79b67d26e0a/marjaw},
  interhash = {040474bcd625e7dcc649bb20c81104d2},
  intrahash = {a4c60811a43da7596716d79b67d26e0a},
  keywords = {},
  note = {cite arxiv:1907.11692},
  timestamp = {2020-12-11T12:52:54.000+0100},
  title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  url = {http://arxiv.org/abs/1907.11692},
  year = 2019
}
@inproceedings{xlm_roberta,
    title = "Unsupervised Cross-lingual Representation Learning at Scale",
    author = "Conneau, Alexis  and
      Khandelwal, Kartikay  and
      Goyal, Naman  and
      Chaudhary, Vishrav  and
      Wenzek, Guillaume  and
      Guzm{\'a}n, Francisco  and
      Grave, Edouard  and
      Ott, Myle  and
      Zettlemoyer, Luke  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.747",
    doi = "10.18653/v1/2020.acl-main.747",
    pages = "8440--8451",
    abstract = "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.",
}



@inproceedings{distilbert,
  title="DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  booktitle={NeurIPS EMC2 Workshop},
  year={2019}
}


@inproceedings{huang2023chatgpt,
author = {Huang, Fan and Kwak, Haewoon and An, Jisun},
title = "Is ChatGPT Better than Human Annotators? Potential and Limitations of ChatGPT in Explaining Implicit Hate Speech",
year = {2023},
isbn = {9781450394192},
publisher = {Association for Computing Machinery},
url = {https://doi.org/10.1145/3543873.3587368},
doi = {10.1145/3543873.3587368},
abstract = {Recent studies have alarmed that many online hate speeches are implicit. With its subtle nature, the explainability of the detection of such hateful speech has been a challenging problem. In this work, we examine whether ChatGPT can be used for providing natural language explanations (NLEs) for implicit hateful speech detection. We design our prompt to elicit concise ChatGPT-generated NLEs and conduct user studies to evaluate their qualities by comparison with human-written NLEs. We discuss the potential and limitations of ChatGPT in the context of implicit hateful speech research.},
booktitle = {Companion Proceedings of the ACM Web Conference 2023},
pages = {294–297},
numpages = {4},
keywords = {Large Language Models, Hate Speech, Human Annotation, Natural Language Explanation, Toxicity Detection, ChatGPT},
location = {Austin, TX, USA},
series = {WWW '23 Companion}
}

@inproceedings{mikolov2013linguistic,
  title="Linguistic regularities in continuous space word representations",
  author={Mikolov, Tom{\'a}{\v{s}} and Yih, Wen-tau and Zweig, Geoffrey},
  booktitle={Proceedings of the 2013 conference of the north american chapter of the association for computational linguistics: Human language technologies},
  pages={746--751},
  year={2013}
}


@inproceedings{pennington-etal-2014-glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}

@inproceedings{elmo,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew E.  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@inproceedings{wang-etal-2021-want-reduce,
    title = "Want To Reduce Labeling Cost? {GPT}-3 Can Help",
    author = "Wang, Shuohang  and
      Liu, Yang  and
      Xu, Yichong  and
      Zhu, Chenguang  and
      Zeng, Michael",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.354",
    doi = "10.18653/v1/2021.findings-emnlp.354",
    pages = "4195--4205",
    abstract = "Data annotation is a time-consuming and labor-intensive process for many NLP tasks. Although there exist various methods to produce pseudo data labels, they are often task-specific and require a decent amount of labeled data to start with. Recently, the immense language model GPT-3 with 170 billion parameters has achieved tremendous improvement across many few-shot learning tasks. In this paper, we explore ways to leverage GPT-3 as a low-cost data labeler to train other models. We find that to make the downstream model achieve the same performance on a variety of NLU and NLG tasks, it costs 50{\%} to 96{\%} less to use labels from GPT-3 than using labels from humans. Furthermore, we propose a novel framework of combining pseudo labels from GPT-3 with human labels, which leads to even better performance. These results present a cost-effective data labeling methodology that is generalizable to many practical applications.",
}

@misc{wu2016google,
      title="Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", 
      author={Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and others},
      year={2016},
      eprint={1609.08144},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{sennrich2015neural,
      title="Neural Machine Translation of Rare Words with Subword Units", 
      author={Rico Sennrich and Barry Haddow and Alexandra Birch},
      year={2016},
      eprint={1508.07909},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@article{gpt,
  title="Improving language understanding by generative pre-training",
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  journal = {OpenAI Blog},
  publisher={OpenAI}
}

@article{gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{infersent_bilstm,
    title = "Supervised Learning of Universal Sentence Representations from Natural Language Inference Data",
    author = {Conneau, Alexis  and
      Kiela, Douwe  and
      Schwenk, Holger  and
      Barrault, Lo{\"\i}c  and
      Bordes, Antoine},
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1070",
    doi = "10.18653/v1/D17-1070",
    pages = "670--680",
    abstract = "Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available.",
}

@book{statistical_nlp_naive_bayes,
  title={Foundations of statistical natural language processing},
  author={Manning, Christopher and Schutze, Hinrich},
  year={1999},
  publisher={MIT press}
}


@Inbook{Shen2009_text_classification_formal_definition,
author="Shen, Dou", 
title="Text Categorization",
bookTitle="Encyclopedia of Database Systems",
year="2009",
publisher="Springer US",
address="Boston, MA",
pages="3041--3044",
isbn="978-0-387-39940-9",
doi="10.1007/978-0-387-39940-9_414",
url="https://doi.org/10.1007/978-0-387-39940-9_414"
}

@inproceedings{barbaresi-2021-trafilatura,
  title = {{Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction}},
  author = "Barbaresi, Adrien",
  booktitle = "Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations",
  pages = "122--131",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2021.acl-demo.15",
  year = 2021,
}


@article{agreement_measures,
    title = "Survey Article: Inter-Coder Agreement for Computational Linguistics",
    author = "Artstein, Ron  and
      Poesio, Massimo",
    journal = "Computational Linguistics",
    volume = "34",
    number = "4",
    year = "2008",
    url = "https://aclanthology.org/J08-4004",
    doi = "10.1162/coli.07-034-R2",
    pages = "555--596",
}


@misc{convention_cadre_pesv,
    title = {Convention cadre portant d\'efinition et organisation de la Plateforme national d'epid\'emiosurveillance en sant\'e v\'eg\'etale},
    author = {{Minist\`ere de L'Agriculture et de L'Alimentation}},
    howpublished = {Convention cadre},
    year = {2018},
    note = {Direction g\'en\'erale de l'Alimentation},
}


@inproceedings{nltk,
    title = "{NLTK}: The Natural Language Toolkit",
    author = "Bird, Steven  and
      Loper, Edward",
    booktitle = "Proceedings of the {ACL} Interactive Poster and Demonstration Sessions",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P04-3031",
    pages = "214--217",
}


@article{survey_general_text_classificaiton,
author = {Li, Qian and Peng, Hao and Li, Jianxin and Xia, Congying and Yang, Renyu and Sun, Lichao and Yu, Philip S. and He, Lifang},
title = {A Survey on Text Classification: From Traditional to Deep Learning},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {2157-6904},
url = {https://doi.org/10.1145/3495162},
doi = {10.1145/3495162},
abstract = {Text classification is the most fundamental and essential task in natural language processing. The last decade has seen a surge of research in this area due to the unprecedented success of deep learning. Numerous methods, datasets, and evaluation metrics have been proposed in the literature, raising the need for a comprehensive and updated survey. This paper fills the gap by reviewing the state-of-the-art approaches from 1961 to 2021, focusing on models from traditional models to deep learning. We create a taxonomy for text classification according to the text involved and the models used for feature extraction and classification. We then discuss each of these categories in detail, dealing with both the technical developments and benchmark datasets that support tests of predictions. A comprehensive comparison between different techniques, as well as identifying the pros and cons of various evaluation metrics are also provided in this survey. Finally, we conclude by summarizing key implications, future research directions, and the challenges facing the research area.},
journal = {ACM Trans. Intell. Syst. Technol.},
month = {apr},
articleno = {31},
numpages = {41},
keywords = {text classification, traditional models, evaluation metrics, Deep learning, challenges}
}


@article{scikit-learn,
 title="Scikit-learn: Machine Learning in {P}ython",
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{auc_definition,
  title="The meaning and use of the area under a receiver operating characteristic ({ROC}) curve.",
  author={Hanley, James A and McNeil, Barbara J},
  journal={Radiology},
  volume={143},
  number={1},
  pages={29--36},
  year={1982}
}


@misc{deep_learning_introduction,
      title="Dive into {Deep} {Learning}", 
      author={Aston Zhang and Zachary C. Lipton and Mu Li and Alexander J. Smola},
      year={2023},
      eprint={2106.11342},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      howpublished = {\url{https://d2l.ai/}},
}


@article{padiweb3,
title = {PADI-web 3.0: A new framework for extracting and disseminating fine-grained information from the news for animal disease surveillance},
journal = {One Health},
volume = {13},
pages = {100357},
year = {2021},
issn = {2352-7714},
doi = {https://doi.org/10.1016/j.onehlt.2021.100357},
url = {https://www.sciencedirect.com/science/article/pii/S2352771421001476},
author = {Sarah Valentin and Elena Arsevska and Julien Rabatel and Sylvain Falala and Alizé Mercier and Renaud Lancelot and Mathieu Roche},
keywords = {Animal disease surveillance, Software, Text mining},
abstract = {PADI-web (Platform for Automated extraction of animal Disease Information from the web) is a biosurveillance system dedicated to monitoring online news sources for the detection of emerging animal infectious diseases. PADI-web has collected more than 380,000 news articles since 2016. Compared to other existing biosurveillance tools, PADI-web focuses specifically on animal health and has a fully automated pipeline based on machine-learning methods. This paper presents the new functionalities of PADI-web based on the integration of: (i) a new fine-grained classification system, (ii) automatic methods to extract terms and named entities with text-mining approaches, (iii) semantic resources for indexing keywords and (iv) a notification system for end-users. Compared to other biosurveillance tools, PADI-web, which is integrated in the French Platform for Animal Health Surveillance (ESA Platform), offers strong coverage of the animal sector, a multilingual approach, an automated information extraction module and a notification tool configurable according to end-user needs.}
}




@misc{wolfram_llm, 
title = {What Is {ChatGPT} Doing and Why Does It Work?},
author = {Wolfram, Stephen},
howpublished={\url{https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/}}, 
journal={Stephen Wolfram Writings}, 
year={2023}, 
month=Feb
} 


@inproceedings{choubert,
  title={{ChouBERT}: Pre-training {French} Language Model for Crowdsensing with {Tweets} in Phytosanitary Context},
  author={Jiang, Shufan and Angarita, Rafael and Cormier, St{\'e}phane and Orensanz, Julien and Rousseaux, Francis},
  booktitle={International Conference on Research Challenges in Information Science},
  pages={653--661},
  year={2022},
  organization={Springer}
}



@article{biobert,
  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  journal={Bioinformatics},
  volume={36},
  number={4},
  pages={1234--1240},
  year={2020},
  publisher={Oxford University Press}
}


@inproceedings{transformers_hf_library,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Wolf, Thomas  and
      Debut, Lysandre  and
      Sanh, Victor  and
      Chaumond, Julien  and
      Delangue, Clement  and
      Moi, Anthony  and
      Cistac, Pierric  and
      Rault, Tim  and
      Louf, Remi  and
      Funtowicz, Morgan  and
      Davison, Joe  and
      Shleifer, Sam  and
      von Platen, Patrick  and
      Ma, Clara  and
      Jernite, Yacine  and
      Plu, Julien  and
      Xu, Canwen  and
      Le Scao, Teven  and
      Gugger, Sylvain  and
      Drame, Mariama  and
      Lhoest, Quentin  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.6",
    doi = "10.18653/v1/2020.emnlp-demos.6",
    pages = "38--45",
    abstract = "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.",
}



@misc{blurb_biomedical_ranking, title={Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing}, volume={3}, url={http://dx.doi.org/10.1145/3458754}, DOI={10.1145/3458754}, abstractNote={<jats:p> Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition. To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding &amp; Reasoning Benchmark) at <jats:ext-link xmlns:xlink="http://www.w3.org/1999/xlink" ext-link-type="url" xlink:href="https://aka.ms/BLURB">https://aka.ms/BLURB</jats:ext-link> . </jats:p>}, number={1}, journal={ACM Transactions on Computing for Healthcare}, publisher={Association for Computing Machinery (ACM)}, author={Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung}, year={2021}, month=Oct, pages={1–23}, language={en}} 

@InProceedings{ccnews_dataset,
  author     = {Hamborg, Felix and Meuschke, Norman and Breitinger, Corinna and Gipp, Bela},
  title      = {news-please: A Generic News Crawler and Extractor},
  year       = {2017},
  booktitle  = {Proceedings of the 15th International Symposium of Information Science},
  location   = {Berlin},
  doi        = {10.5281/zenodo.4120316},
  pages      = {218--223},
  month      = mar
}

@misc{openwebtext,
    title={OpenWebText Corpus},
    author={Gokaslan,Aaron and Cohen, Vanya and Pavlick, Ellie and Tellex, Stefanie},
    howpublished={\url{http://Skylion007.github.io/OpenWebTextCorpus}},
    year={2019}
}

@article{stories_dataset,
  title={A Simple Method for Commonsense Reasoning},
  author={Trieu H. Trinh and Quoc V. Le},
  journal={ArXiv},
  year={2018},
  volume={abs/1806.02847},
  url={https://api.semanticscholar.org/CorpusID:47015717}
}

@inproceedings{sentencepiece,
    title = "{S}entence{P}iece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
    author = "Kudo, Taku  and
      Richardson, John",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-2012",
    doi = "10.18653/v1/D18-2012",
    pages = "66--71",
    abstract = "This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at \url{https://github.com/google/sentencepiece}.",
}


@article{heat_index,
 ISSN = {00218952, 2163534X},
 URL = {http://www.jstor.org/stable/26179216},
 abstract = {Using as bases the amount of clothing needed to achieve thermal comfort and the reduction in the skin's resistance needed to obtain thermal equilibrium, the relative sultriness of warm-humid and hot-arid summer climates is assessed. Conditions of equal sultriness are referred to a vapor pressure of 1.6 kPa in order to prepare a table of apparent temperature corresponding to summer temperatures and humidities.},
 author = {R. G. Steadman},
 journal = {Journal of Applied Meteorology (1962-1982)},
 number = {7},
 pages = {861--873},
 publisher = {American Meteorological Society},
 title = {The Assessment of Sultriness. Part I: A Temperature-Humidity Index Based on Human Physiology and Clothing Science},
 urldate = {2023-08-02},
 volume = {18},
 year = {1979}
}

@article{drawing_nns, doi = {10.21105/joss.00747}, url = {https://doi.org/10.21105/joss.00747}, year = {2019}, publisher = {The Open Journal}, volume = {4}, number = {33}, pages = {747}, author = {Alexander LeNail}, title = {NN-SVG: Publication-Ready Neural Network Architecture Schematics}, journal = {Journal of Open Source Software} } 


 @article{revolution_bert, 
title={BERT: A Review of Applications in Natural Language Processing and Understanding}, url={https://arxiv.org/abs/2103.11943}, 
DOI={10.48550/ARXIV.2103.11943}, 
abstractNote={In this review, we describe the application of one of the most popular deep learning-based language models - BERT. The paper describes the mechanism of operation of this model, the main areas of its application to the tasks of text analytics, comparisons with similar models in each task, as well as a description of some proprietary models. In preparing this review, the data of several dozen original scientific articles published over the past few years, which attracted the most attention in the scientific community, were systematized. This survey will be useful to all students and researchers who want to get acquainted with the latest advances in the field of natural language text analysis.}, 
journal={arXiv}, 
author={Koroteev, M. V.}, 
year={2021}} 



@article{attention_distilled,
  author = {Olah, Chris and Carter, Shan},
  title = {Attention and Augmented Recurrent Neural Networks},
  journal = {Distill},
  year = {2016},
  url = {http://distill.pub/2016/augmented-rnns},
  doi = {10.23915/distill.00001}
}

@inproceedings{attention_for_translation,
  author       = {Dzmitry Bahdanau and
                  Kyunghyun Cho and
                  Yoshua Bengio},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
  url          = {http://arxiv.org/abs/1409.0473},
  timestamp    = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{attention_is_all_you_need,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}


@article{pet_paper,
  title={Exploiting Cloze Questions for Few-Shot Text Classification and Natural Language Inference},
  author={Timo Schick and Hinrich Schütze},
  journal={Computing Research Repository},
  volume={arXiv:2001.07676},
  url={http://arxiv.org/abs/2001.07676},
  year={2020}
}


@article{beautiful_soup_4,
  title={Beautiful Soup 4 documentation},
  author={Richardson, Leonard},
  journal={dec},
  year={2019},
  howpublished = {\url{https://www.crummy.com/software/BeautifulSoup/}}
} 



@misc{pandas,
    author       = {The Pandas Development Team},
    title        = {pandas-dev/pandas: Pandas},
    month        = feb,
    year         = 2020,
    publisher    = {Zenodo},
    doi          = {\url{10.5281/zenodo.3509134}},
    howpublished = {\url{https://doi.org/10.5281/zenodo.3509134}}
}


@inproceedings{alvisnlp,
  TITLE = {{Interoperability of corpus processing workflow engines: the case of. AlvisNLP/ML in OpenMinTeD}},
  AUTHOR = {Ba, Mouhamadou and Bossy, Robert},
  URL = {https://hal.science/hal-01455853},
  BOOKTITLE = {{Meeting of working Group Medicago sativa}},
  ADDRESS = {Portoroz, Slovenia},
  PAGES = {np},
  YEAR = {2016},
  MONTH = May,
  KEYWORDS = {Text-mining ; platform ; interoperability ; Natural Language Processing ; Workflow ; Processing Workflows ; Software Interoperability},
  PDF = {https://hal.science/hal-01455853/file/interoperability_1.pdf},
  HAL_ID = {hal-01455853},
  HAL_VERSION = {v1},
}


@article{flash_attention, title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}, url={https://arxiv.org/abs/2205.14135}, DOI={10.48550/ARXIV.2205.14135}, journal={arXiv},  author={Dao, Tri and Fu, Daniel Y. and Ermon, Stefano and Rudra, Atri and Ré, Christopher}, year={2022}} 


@article{ai_for_agriculture,
title = {Understanding the potential applications of Artificial Intelligence in Agriculture Sector},
journal = {Advanced Agrochem},
volume = {2},
number = {1},
pages = {15-30},
year = {2023},
issn = {2773-2371},
doi = {https://doi.org/10.1016/j.aac.2022.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S277323712200020X},
author = {Mohd Javaid and Abid Haleem and Ibrahim Haleem Khan and Rajiv Suman},
keywords = {Artificial intelligence, Application, Agriculture, Farming, Information, Machine learning},
abstract = {Artificial Intelligence (AI) has been extensively applied in farming recently. To cultivate healthier crops, manage pests, monitor soil and growing conditions, analyse data for farmers, and enhance other management activities of the food supply chain, the agriculture sector is turning to AI technology. It makes it challenging for farmers to choose the ideal time to plant seeds. AI helps farmers choose the optimum seed for a particular weather scenario. It also offers data on weather forecasts. AI-powered solutions will help farmers produce more with fewer resources, increase crop quality, and hasten product time to reach the market. AI aids in understanding soil qualities. AI helps farmers by suggesting the nutrients they should apply to increase the quality of the soil. AI can help farmers choose the optimal time to plant their seeds. Intelligent equipment can calculate the spacing between seeds and the maximum planting depth. An AI-powered system known as a health monitoring system provides farmers with information on the health of their crops and the nutrients that need to be given to enhance yield quality and quantity. This study identifies and analyses relevant articles on AI for Agriculture. Using AI, farmers can now access advanced data and analytics tools that will foster better farming, improve efficiencies, and reduce waste in biofuel and food production while minimising the negative environmental impacts. AI and Machine Learning (ML) have transformed various industries, and the AI wave has now reached the agriculture sector. Companies are developing several technologies to make monitoring farmers' crop and soil health easier. Hyperspectral imaging and 3D laser scanning are the leading AI-based technologies that can help ensure crop health. These AI-powered technologies collect precise data on the health of the crops in greater volume for analysis. This paper studied AI and its need in Agriculture. The process of AI in Agriculture and some Agriculture parameters monitored by AI are briefed. Finally, we identified and discussed the significant applications of AI in agriculture.}
}


@article{sensors_for_agriculture,
title = {Artificial intelligence - enabled soft sensor and internet of things for sustainable agriculture using ensemble deep learning architecture},
journal = {Computers and Electrical Engineering},
volume = {102},
pages = {108128},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108128},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622003780},
author = {Anupong Wongchai and Surendra Kumar Shukla and Mohammed Altaf Ahmed and Ulaganathan Sakthi and Mukta Jagdish and Ravi kumar},
keywords = {Agriculture, Predictive maintenance, CPS, Soft sensors, Deep learning, Feature representation, Classification},
abstract = {IoT (Internet of things) and Artificial Intelligence (AI), as well as other advanced computing technologies, have long been used in agriculture.AI-enabled sensors function as smart sensors and IoT has made various types of sensor-based equipment in the field of agriculture. This research proposes novel techniques in AI technique based soft sensor integrated with remote sensing model using deep learning architectures. The input has been pre-processed to recognize the missing value, data cleaning and noise removal from the image which is collected from the agricultural land. The feature representation has been carried out usingweight-optimized neural network with maximum likelihood (WONN_ML). after representing the features, classification process has been carried out using ensemble architecture of stacked auto-encoder and kernel-based convolution network (SAE_KCN). The experimental results have been done for various crops in terms of computational time of 56%, accuracy 98%, precision of 85.5%, recall of 89.9% and F-1 score of 86% by proposed technique.}
}


@article{iot_for_agriculture,
title = {A smart agriculture IoT system based on deep reinforcement learning},
journal = {Future Generation Computer Systems},
volume = {99},
pages = {500-507},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.04.041},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19307277},
author = {Fanyu Bu and Xin Wang},
keywords = {Deep reinforcement learning, Smart agriculture IoT, Edge computing, Cloud computing},
abstract = {Smart agriculture systems based on Internet of Things are the most promising to increase food production and reduce the consumption of resources like fresh water. In this study, we present a smart agriculture IoT system based on deep reinforcement learning which includes four layers, namely agricultural data collection layer, edge computing layer, agricultural data transmission layer, and cloud computing layer. The presented system integrates some advanced information techniques, especially artificial intelligence and cloud computing, with agricultural production to increase food production. Specially, the most advanced artificial intelligence model, deep reinforcement learning is combined in the cloud layer to make immediate smart decisions such as determining the amount of water needed to be irrigated for improving crop growth environment. We present several representative deep reinforcement learning models with their broad applications. Finally, we talk about the open challenges and the potential applications of deep reinforcement learning in smart agriculture IoT systems.}
}


@misc{social_media_disease_surveillance_2015_no_bert, 
title={Using Social Media for Actionable Disease Surveillance and Outbreak Management: A Systematic Literature Review}, volume={10}, url={http://dx.doi.org/10.1371/journal.pone.0139701}, DOI={10.1371/journal.pone.0139701}, number={10}, journal={PLOS ONE}, publisher={Public Library of Science (PLoS)}, author={Charles-Smith, Lauren E. and Reynolds, Tera L. and Cameron, Mark A. and Conway, Mike and Lau, Eric H. Y. and Olsen, Jennifer M. and Pavlin, Julie A. and Shigematsu, Mika and Streichert, Laura C. and Suda, Katie J. and Corley, Courtney D.}, editor={Braunstein, Lidia Adriana}, year={2015}, pages={e0139701}, language={en}
}


@inproceedings{twitter_detect_influenza_epidemics_2011,
    title = "{T}witter Catches The Flu: Detecting Influenza Epidemics using {T}witter",
    author = "Aramaki, Eiji  and
      Maskawa, Sachiko  and
      Morita, Mizuki",
    booktitle = "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2011",
    address = "Edinburgh, Scotland, UK.",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D11-1145",
    pages = "1568--1576",
}



@unpublished{data_mining_for_plant_health,
  TITLE = {{Open Data Platform for Knowledge Access in Plant Health Domain : VESPA Mining}},
  AUTHOR = {Turenne, Nicolas and Andro, Mathieu and Corbi{\`e}re, Roselyne and Phan, Tien T.},
  URL = {https://hal.science/hal-01145489},
  NOTE = {working paper or preprint},
  TYPE = {Other},
  PAGES = {5 p.},
  YEAR = {2015},
  KEYWORDS = {Document Structure ; End-User Web Platform ; OCR correction ; Data Sciences ; Open Data ; Plant Health ; Text-Mining},
  PDF = {https://hal.science/hal-01145489/file/1504.06077.pdf},
  HAL_ID = {hal-01145489},
  HAL_VERSION = {v1},
}



@article{limitations_of_social_media_for_biosecurity_events,
    doi = {10.1371/journal.pone.0172457},
    author = {Welvaert, Marijke AND Al-Ghattas, Omar AND Cameron, Mark AND Caley, Peter},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Limits of use of social media for monitoring biosecurity events},
    year = {2017},
    month = {02},
    volume = {12},
    url = {https://doi.org/10.1371/journal.pone.0172457},
    pages = {1-17},
    abstract = {Compared to applications that trigger massive information streams, like earthquakes and human disease epidemics, the data input for agricultural and environmental biosecurity events (ie. the introduction of unwanted exotic pests and pathogens), is expected to be sparse and less frequent. To investigate if Twitter data can be useful for the detection and monitoring of biosecurity events, we adopted a three-step process. First, we confirmed that sightings of two migratory species, the Bogong moth (Agrotis infusa) and the Common Koel (Eudynamys scolopaceus) are reported on Twitter. Second, we developed search queries to extract the relevant tweets for these species. The queries were based on either the taxonomic name, common name or keywords that are frequently used to describe the species (symptomatic or syndromic). Third, we validated the results using ground truth data. Our results indicate that the common name queries provided a reasonable number of tweets that were related to the ground truth data. The taxonomic query resulted in too small datasets, while the symptomatic queries resulted in large datasets, but with highly variable signal-to-noise ratios. No clear relationship was observed between the tweets from the symptomatic queries and the ground truth data. Comparing the results for the two species showed that the level of familiarity with the species plays a major role. The more familiar the species, the more stable and reliable the Twitter data. This clearly presents a problem for using social media to detect the arrival of an exotic organism of biosecurity concern for which public is unfamiliar.},
    number = {2},

}


@INPROCEEDINGS{ai_for_covid,

  author={Nadeem, Osama and Saeed, Muhammad Shajee and Tahir, Muhammad Ali and Mumtaz, Rafia},

  booktitle={2020 IEEE 17th International Conference on Smart Communities: Improving Quality of Life Using ICT, IoT and AI (HONET)}, 

  title={A Survey of Artificial Intelligence and Internet of Things (IoT) based approaches against Covid-19}, 

  year={2020},

  volume={},

  number={},

  pages={214-218},

  doi={10.1109/HONET50430.2020.9322829}}


@INPROCEEDINGS{social_media_crop_health_monitoring,

  author={Shankar, Priyamvada and Bitter, Christian and Liwicki, Marcus},

  booktitle={2020 IEEE / ITU International Conference on Artificial Intelligence for Good (AI4G)}, 

  title={Digital Crop Health Monitoring by Analyzing Social Media Streams}, 

  year={2020},

  volume={},

  number={},

  pages={87-94},

  doi={10.1109/AI4G50087.2020.9310985}}



@inproceedings{martin-etal-2020-camembert,
    title = "{C}amem{BERT}: a Tasty {F}rench Language Model",
    author = "Martin, Louis  and
      Muller, Benjamin  and
      Ortiz Su{\'a}rez, Pedro Javier  and
      Dupont, Yoann  and
      Romary, Laurent  and
      de la Clergerie, {\'E}ric  and
      Seddah, Djam{\'e}  and
      Sagot, Beno{\^\i}t",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.645",
    doi = "10.18653/v1/2020.acl-main.645",
    pages = "7203--7219",
    abstract = "Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models {--}in all languages except English{--} very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks.",
}
